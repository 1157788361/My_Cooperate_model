# """ç ”ç©¶ detach å¯¹ è®¡ç®—å›¾çš„å½±å“ã€‚ """
"""å¦‚æœæƒ³è¦è®¡ç®—å„ä¸ªVariableçš„æ¢¯åº¦ï¼Œåªéœ€è°ƒç”¨æ ¹èŠ‚ç‚¹variableçš„backwardæ–¹æ³•ï¼Œautogradä¼šè‡ªåŠ¨æ²¿ç€è®¡ç®—å›¾åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¯ä¸€ä¸ªå¶å­èŠ‚ç‚¹çš„æ¢¯åº¦ã€‚"""
# import torch
# x = torch.tensor([1.0,2.0],requires_grad=True)
# x1 = torch.tensor([3.0,4.0],requires_grad=True)
# y1 = x**2
# y2 = y1.detach()*2
# y3 = y2*2
# y4 = y1 +y3+y2
# y5 = y4**2 + x1
#
# print(y1, y1.requires_grad) # grad_fn=<PowBackward0>
# print(y2, y2.requires_grad) # None
# print(y3, y3.requires_grad) # None
# print(y4, y4.requires_grad) # grad_fn=<AddBackward0>

# x.grad: tensor([ 28., 224.])
# x1.grad: tensor([ 28., 224.])
# y5.backward(torch.ones(y3.shape))
# print('grad')
# print('x.grad:',x.grad) # åªæœ‰å¶å­èŠ‚ç‚¹æœ‰ grad å€¼ã€‚
# print('x1.grad:',x1.grad)
# print('y1.grad:', y1.grad) # éƒ½æ˜¯ x äº§ç”Ÿçš„å¤åˆå‡½æ•°ã€‚å³éƒ½æ˜¯å¶å­èŠ‚ç‚¹ã€‚
# print('y2.grad:', y2.grad) # éå¶å­èŠ‚ç‚¹gradè®¡ç®—å®Œä¹‹åè‡ªåŠ¨æ¸…ç©º
# print('y3.grad:', y3.grad)
# print('y4.grad:', y4.grad)

""" ç ”ç©¶ å°† åŸè®¡ç®—å›¾ä¸­å¶å­èŠ‚ç‚¹å‰¥ç¦»å‡ºæ¥å¹¶é‡æ–°è®¡ç®—ï¼Œæ˜¯å¦å¯¹åŸ è®¡ç®—å›¾çš„ åå‘ä¼ æ’­æœ‰å½±å“ã€‚"""
""" PyTorchä½¿ç”¨çš„æ˜¯åŠ¨æ€å›¾ï¼Œå®ƒçš„è®¡ç®—å›¾åœ¨æ¯æ¬¡å‰å‘ä¼ æ’­æ—¶éƒ½æ˜¯ä»å¤´å¼€å§‹æ„å»ºï¼Œæ‰€ä»¥å®ƒèƒ½å¤Ÿä½¿ç”¨Pythonæ§åˆ¶è¯­å¥ï¼ˆå¦‚forã€ifç­‰ï¼‰æ ¹æ®éœ€æ±‚åˆ›å»ºè®¡ç®—å›¾ã€‚ """
import torch
# ç®€åŒ–ç½‘ç»œï¼Œæ²¡æœ‰ detach å½±å“
x = torch.tensor([1.0,2.0],requires_grad=True)
x1 = torch.tensor([3.0,4.0],requires_grad=True)
y1 = x**2
y2 = y1*2
y3 = y2*2
y4 = y1 +y3+y2
y5 = y4**2 + x1

print('å‰¥ç¦»å‰çš„ è®¡ç®—å›¾çŠ¶æ€')
print(x, x.requires_grad,x.grad)
print(x1, x1.requires_grad,x1.grad)
print(y1, y1.requires_grad,y1.grad) # éå¶å­èŠ‚ç‚¹gradè®¡ç®—å®Œä¹‹åè‡ªåŠ¨æ¸…ç©º
print(y2, y2.requires_grad,y2.grad)
print(y3, y3.requires_grad,y3.grad)
print(y4, y4.requires_grad,y4.grad)
# å¦‚æœ y5.backward(torch.ones(y5.shape)) ï¼Œè¾“å‡ºï¼š
# tensor([1., 2.], requires_grad=True) True tensor([ 196., 1568.])
# tensor([3., 4.], requires_grad=True) True tensor([1., 1.])
# tensor([1., 4.], grad_fn=<PowBackward0>) True None
# tensor([2., 8.], grad_fn=<MulBackward0>) True None
# tensor([ 4., 16.], grad_fn=<MulBackward0>) True None
# tensor([ 7., 28.], grad_fn=<AddBackward0>) True None



y_1 = y3
# å°† å¶å­èŠ‚ç‚¹ä» è®¡ç®—å›¾ä¸­å‰¥ç¦»å‡ºæ¥ã€‚
p_list = [x,x1,y1,y2,y3,y4,y5]
for p in p_list:
    if p.grad is not None:  # è®¡ç®—çš„æ—¶å€™ï¼Œåªæœ‰ å¶å­èŠ‚ç‚¹çš„  p.grad ä¸º not Noneã€‚
        # å½“æˆ‘ä»¬å†è®­ç»ƒç½‘ç»œçš„æ—¶å€™å¯èƒ½å¸Œæœ›ä¿æŒä¸€éƒ¨åˆ†çš„ç½‘ç»œå‚æ•°ä¸å˜ï¼Œåªå¯¹å…¶ä¸­ä¸€éƒ¨åˆ†çš„å‚æ•°è¿›è¡Œè°ƒæ•´ï¼›
        # æˆ–è€…åª è®­ç»ƒéƒ¨åˆ†åˆ†æ”¯ç½‘ç»œï¼Œå¹¶ä¸è®©å…¶æ¢¯åº¦å¯¹ä¸»ç½‘ç»œçš„æ¢¯åº¦é€ æˆå½±å“ï¼Œ
        # è¿™æ—¶å€™æˆ‘ä»¬å°±éœ€è¦ä½¿ç”¨detach()å‡½æ•°æ¥åˆ‡æ–­ä¸€äº›åˆ†æ”¯çš„åå‘ä¼ æ’­
        p.grad.detach_()
        p.grad.zero_()

# å‰¥ç¦»åçš„å„ä¸ªè®¡ç®—å›¾çš„èŠ‚ç‚¹çŠ¶æ€ï¼š
# æ— è®ºæ˜¯å¦ç»å†ï¼šy5.backward(torch.ones(y5.shape))ï¼Œè¾“å‡ºç»“æœä¸å˜ã€‚å³ grad_fn è‡ªåŠ¨è®°å½• ä¸­é—´èŠ‚ç‚¹çš„ è®¡ç®—è¿‡ç¨‹ã€‚å³ åŠ¨æ€æ„å»º è®¡ç®—å›¾ ã€‚
print('detach_and_zero_grad')
print('x.grad:',x,x.grad)
print('x1.grad:',x1,x1.grad)
print('y1.grad:', y1,y1.grad) # éƒ½æ˜¯ x äº§ç”Ÿçš„å¤åˆå‡½æ•°ã€‚å³éƒ½æ˜¯å¶å­èŠ‚ç‚¹ã€‚
print('y2.grad:', y2,y2.grad)
print('y3.grad:', y3,y3.grad)
print('y4.grad:', y4,y4.grad)
print('y5.grad:', y5,y5.grad)

# è¾“å‡ºï¼š
# detach_and_zero_grad
# x.grad: tensor([1., 2.], requires_grad=True) None
# x1.grad: tensor([3., 4.], requires_grad=True) None
# y1.grad: tensor([1., 4.], grad_fn=<PowBackward0>) None
# y2.grad: tensor([2., 8.], grad_fn=<MulBackward0>) None
# y3.grad: tensor([ 4., 16.], grad_fn=<MulBackward0>) None
# y4.grad: tensor([ 7., 28.], grad_fn=<AddBackward0>) None
# y5.grad: tensor([ 52., 788.], grad_fn=<AddBackward0>) None
# å¯ä»¥çœ‹åˆ°å‰¥ç¦»åçš„ ç‚¹ä¸­ï¼Œè®¡ç®—å›¾ä¾ç„¶å­˜åœ¨ã€‚ åªæ˜¯æ¢¯åº¦å€¼æ¶ˆå¤±ã€‚ æ¢¯åº¦å€¼ä¾ç„¶å¯ä»¥æ ¹æ® è‡ªèº«å€¼ ä¸ è®¡ç®—å›¾ è¿›è¡Œæ¢¯åº¦è®¡ç®—ã€‚

# x = torch.tensor([1.0,2.0],requires_grad=True)
# x1 = torch.tensor([3.0,4.0],requires_grad=True) # å¦‚æœç›´æ¥ç”¨ ä¸Šè®¡ç®—å›¾ä½¿ç”¨è¿‡çš„ å¶å­èŠ‚ç‚¹ï¼Œåˆ™ä¼šå°† åŸè®¡ç®—å›¾çš„ å¶å­èŠ‚ç‚¹ç ´åæ‰ï¼Œå¯¼è‡´ backward  å¶å­èŠ‚ç‚¹çš„ grad == Noneï¼Œè™½ç„¶è¿›è¡Œäº†p.grad.detach_() p.grad.zero_()

# x_det = torch.tensor([1.0,2.0],requires_grad=True)
# x1_det = torch.tensor([3.0,4.0],requires_grad=True)
# x = x_det
# x1 = x1_det # åå­—ç›¸åŒéƒ½ä¼šå‡ºç° x.grad == None

"""å®éªŒ å½“ å¶å­èŠ‚ç‚¹å˜é‡åä¸åŒï¼Œä¸­é—´å˜é‡åç›¸åŒï¼Œä¸ä¼šå½±å“ backward çš„ç»“æœ"""
# åªæœ‰è¿™æ—¶ï¼ˆå¶å­èŠ‚ç‚¹çš„å˜é‡åä¸åŒï¼‰ï¼Œæ‰ä¼šåœ¨ y5.backward æ—¶ï¼Œå›æº¯åˆ° å·²ç»è®°å½•çš„ x,x1 çš„å¶å­èŠ‚ç‚¹ä¸Šã€‚
# x_det = torch.tensor([1.0,2.0],requires_grad=True)
# x1_det = torch.tensor([3.0,4.0],requires_grad=True)
# # ä¸ä¸Šä¸€ä¸ªè®¡ç®—å›¾çš„è®¡ç®—æ–¹å¼ä¸ä¸€æ ·
# y1 = x_det*2
# y2 = y1**2
# y3 = y2*2
# y4 = y1 +y3+y2
# y6 = y4**2 + x1_det*2
#
# y_2 = y3
#
# # åœ¨å°†è®¡ç®—å›¾ä¸­å¶å­èŠ‚ç‚¹å‰¥ç¦»åï¼Œä»ç„¶å¯ä»¥åå‘ä¼ æ’­ã€‚å³ ç»“æœ y5 çš„ grad_fn ä¿å­˜äº† å…¶åå‘ä¼ æ’­çš„å‡½æ•°ï¼Œä¸ å¯è¿½æº¯çš„ä¸­é—´è®¡ç®—è¿‡ç¨‹(å‚æ•°åå¯è¢«å…¶ä»–è®¡ç®—å›¾åˆ©ç”¨ï¼Œå³y1,y2,y3å¯ä»¥å¤šå›¾ä½¿ç”¨ã€‚)ï¼Œ
# # å’Œ å¶å­èŠ‚ç‚¹çš„åå­—(x,x1)ã€‚ï¼ˆå¶å­èŠ‚ç‚¹è¢«é‡ç½® æˆ–è€… å‚ä¸å…¶ä»–è®¡ç®—å›¾ï¼Œä¼šå¯¼è‡´åå‘ä¼ æ’­æ— æ³•ç»§ç»­ã€‚ï¼‰ åº”è¯¥æ˜¯ åœ¨åŒä¸€å­˜å‚¨ç©ºé—´ï¼Œäº§ç”Ÿå†²çªã€‚ è€ƒè™‘ clean_image_l.detach().clone()
# y5.backward(torch.ones(y5.shape))
# print('y5 åœ¨ detach_and_zero åï¼Œä¾ç„¶å¯¹åŸç»“æœè¿›è¡Œ backward')
# print('grad')
# print('x.grad:',x,x.grad) # x.grad: tensor([1., 2.], requires_grad=True) tensor([ 196., 1568.])
# print('x1.grad:',x1,x1.grad) # x1.grad: tensor([3., 4.], requires_grad=True) tensor([1., 1.])
# print('y1.grad:', y1,y1.grad) # éƒ½æ˜¯ x äº§ç”Ÿçš„å¤åˆå‡½æ•°ã€‚å³éƒ½æ˜¯å¶å­èŠ‚ç‚¹ã€‚
# print('y2.grad:', y2,y2.grad)
# print('y3.grad:', y3,y3.grad)
# print('y4.grad:', y4,y4.grad)

# ä»ç»“æœå¯ä»¥çœ‹åˆ° ä¸Šä¸‹ä¸¤ä¸ªçš„
#
# y6.backward(torch.ones(y5.shape))
# print('grad')
# print('x_det.grad:',x_det,x_det.grad) # x_det.grad: tensor([1., 2.], requires_grad=True) tensor([ 728., 5200.])
# print('x1_det.grad:',x1_det,x1_det.grad) # x1_det.grad: tensor([3., 4.], requires_grad=True) tensor([2., 2.])
# print('y1.grad:', y1,y1.grad) # éƒ½æ˜¯ x äº§ç”Ÿçš„å¤åˆå‡½æ•°ã€‚å³éƒ½æ˜¯å¶å­èŠ‚ç‚¹ã€‚
# print('y2.grad:',y2, y2.grad)
# print('y3.grad:', y3,y3.grad)
# print('y4.grad:', y4,y4.grad)
#
# print('y_1 == y_2 ,{}'.format(y_2==y_1))
# print('y3 == y3 ,{}'.format(y3==y3))
# # y6.backward(torch.ones(y3.shape))  åŒæ ·ä¼šè®¡å…¥ é‡å¤å›¾çš„åå‘ä¼ æ’­ ï¼ˆTrying to backward through the graph a second timeï¼‰

"""ä½¿ç”¨ clone æ“ä½œï¼Œæ›¿æ¢åŸ x1,x ç­‰ å¶å­èŠ‚ç‚¹ ï¼Œå³æ¢ä¸€ä¸ªå­˜å‚¨ç©ºé—´ã€‚"""
x_ = x.detach().clone() # é»˜è®¤ required_grad ä¸º false
x_1 = x1.detach().clone() # å¯ä»¥é€šè¿‡xxx.requires_grad_()å°†é»˜è®¤çš„Flaseä¿®æ”¹ä¸ºTrue
x_.requires_grad_()
x_1.requires_grad_()

y1 = x_*2
y2 = y1**2
y3 = y2*2
y4 = y1 +y3+y2
y6 = y4**2 + x_1*2

y_2 = y3

# åœ¨å°†è®¡ç®—å›¾ä¸­å¶å­èŠ‚ç‚¹å‰¥ç¦»åï¼Œä»ç„¶å¯ä»¥åå‘ä¼ æ’­ã€‚å³ ç»“æœ y5 çš„ grad_fn ä¿å­˜äº† å…¶åå‘ä¼ æ’­çš„å‡½æ•°ï¼Œä¸ å¯è¿½æº¯çš„ä¸­é—´è®¡ç®—è¿‡ç¨‹(å‚æ•°åå¯è¢«å…¶ä»–è®¡ç®—å›¾åˆ©ç”¨ï¼Œå³y1,y2,y3å¯ä»¥å¤šå›¾ä½¿ç”¨ã€‚)ï¼Œ
# å’Œ å¶å­èŠ‚ç‚¹çš„åå­—(x,x1)ã€‚ï¼ˆå¶å­èŠ‚ç‚¹è¢«é‡ç½® æˆ–è€… å‚ä¸å…¶ä»–è®¡ç®—å›¾ï¼Œä¼šå¯¼è‡´åå‘ä¼ æ’­æ— æ³•ç»§ç»­ã€‚ï¼‰ åº”è¯¥æ˜¯ åœ¨åŒä¸€å­˜å‚¨ç©ºé—´ï¼Œäº§ç”Ÿå†²çªã€‚clean_image_l.detach().clone()
y5.backward(torch.ones(y5.shape))
print('y5 åœ¨ detach_and_zero åï¼Œä¾ç„¶å¯¹åŸç»“æœè¿›è¡Œ backward')
print('grad')
print('x.grad:',x,x.grad) # x.grad: tensor([1., 2.], requires_grad=True) tensor([ 196., 1568.])
print('x1.grad:',x1,x1.grad) # x1.grad: tensor([3., 4.], requires_grad=True) tensor([1., 1.])
print('y1.grad:', y1,y1.grad)
print('y2.grad:', y2,y2.grad)
print('y3.grad:', y3,y3.grad)
print('y4.grad:', y4,y4.grad)

y6.backward(torch.ones(y6.shape))
print('y5 åœ¨ detach_and_zero åï¼Œä¾ç„¶å¯¹åŸç»“æœè¿›è¡Œ backward')
print('grad')
print('x_.grad:',x_,x_.grad) # x_.grad: tensor([1., 2.], requires_grad=True) tensor([ 728., 5200.])
print('x_1.grad:',x_1,x_1.grad) # x_1.grad: tensor([3., 4.], requires_grad=True) tensor([2., 2.])
print('y1.grad:', y1,y1.grad)
print('y2.grad:', y2,y2.grad)
print('y3.grad:', y3,y3.grad)
print('y4.grad:', y4,y4.grad)

"""æ€»ç»“å¦‚ä¸‹ ï¼š 
            1. å½“ä½¿ç”¨ detach å°†ä¸­é—´èŠ‚ç‚¹ä»è®¡ç®—å›¾ä¸­è„±ç¦»æ—¶ï¼Œåç»­çš„è®¡ç®—ä¸­ï¼Œè®¡ç®—å›¾ä¸åœ¨è®°å½•å…¶ grad_fn ï¼Œå³ä¸º None ï¼Œä¹Ÿå³ loss.backward æ—¶ï¼Œå½“è¿½æº¯åˆ° ä¸å…¶ç›¸å…³çš„ä¸­é—´è®¡ç®—æ—¶ï¼Œæ­¤èŠ‚ç‚¹ ä¼šè‡ªåŠ¨è¢«å¿½ç•¥ã€‚ä¸å†è®¡ç®—å…¶æ¢¯åº¦ã€‚
            2. å½“å»ºç«‹å¥½ä¸€ä¸ªè®¡ç®—å›¾åï¼Œå¦‚æœï¼Œå°†å…¶å¶å­èŠ‚ç‚¹å…¨éƒ¨ detach åï¼Œå†åˆ©ç”¨å¶å­èŠ‚ç‚¹é‡æ–°å»ºç«‹æ–°çš„è®¡ç®—å›¾æ—¶ï¼Œ åªè¦ä¸æ”¹å˜ å¶å­èŠ‚ç‚¹åœ¨ detach å‰çš„ å­˜å‚¨ä½ç½®ï¼Œå¦‚åˆ©ç”¨ detach().clone() ï¼Œå‰ä¸€ä¸ªè®¡ç®—å›¾ä¾ç„¶å¯ä»¥è¿›è¡Œ loss.backwardã€‚å¹¶è®¡ç®—æ¢¯åº¦ã€‚
            3. æ¯ä¸€ä¸ªè®¡ç®—å›¾çš„å»ºç«‹ï¼Œä¸­é—´èŠ‚ç‚¹éƒ½æ˜¯é›¶æ—¶åˆ›å»ºï¼Œå°½ç®¡ä½¿ç”¨ç›¸åŒå‚æ•°åã€‚å­˜å‚¨ä½ç½®ä¸é‡å¤åˆ©ç”¨ã€‚è¿™ä¹Ÿæ˜¯è™½ç„¶åˆ›å»ºäº†å‡ ä¸ªæ–°çš„è®¡ç®—å›¾ï¼Œä½†æ˜¯æ¯ä¸€ä¸ªéƒ½å¯ä»¥è¿›è¡Œè‡ªå·±çš„ loss.backward å›æº¯ã€‚"""

"""åœ¨PyTorchä¸­è®¡ç®—å›¾çš„ç‰¹ç‚¹å¯æ€»ç»“å¦‚ä¸‹ï¼š
    1.autogradæ ¹æ®ç”¨æˆ·å¯¹variableçš„æ“ä½œæ„å»ºå…¶è®¡ç®—å›¾ã€‚å¯¹å˜é‡çš„æ“ä½œæŠ½è±¡ä¸ºFunctionã€‚
    2.å¯¹äºé‚£äº›ä¸æ˜¯ä»»ä½•å‡½æ•°(Function)çš„è¾“å‡ºï¼Œç”±ç”¨æˆ·åˆ›å»ºçš„èŠ‚ç‚¹ç§°ä¸ºå¶å­èŠ‚ç‚¹ï¼Œå¶å­èŠ‚ç‚¹çš„grad_fnä¸ºNoneã€‚å¶å­èŠ‚ç‚¹ä¸­éœ€è¦æ±‚å¯¼çš„variableï¼Œå…·æœ‰AccumulateGradæ ‡è¯†ï¼Œå› å…¶æ¢¯åº¦æ˜¯ç´¯åŠ çš„ã€‚
    3.variableé»˜è®¤æ˜¯ä¸éœ€è¦æ±‚å¯¼çš„ï¼Œå³requires_gradå±æ€§é»˜è®¤ä¸ºFalseï¼Œå¦‚æœæŸä¸€ä¸ªèŠ‚ç‚¹requires_gradè¢«è®¾ç½®ä¸ºTrueï¼Œé‚£ä¹ˆæ‰€æœ‰ä¾èµ–å®ƒçš„èŠ‚ç‚¹requires_gradéƒ½ä¸ºTrueã€‚(å¦‚ï¼š a = b +c ï¼Œåˆ™ä¸º a ä¾èµ–  b å’Œ c ï¼Œ å³è®¡ç®—å›¾ä¸­ï¼Œä¸€ä¸ªèŠ‚ç‚¹requires_grad ä¸º Trueï¼Œæ‰€æœ‰å»ºç«‹åœ¨å…¶ä¹‹ä¸Šçš„è¿ç®—ï¼Œrequires_grad éƒ½ä¸º True.)
    4.variableçš„volatileå±æ€§é»˜è®¤ä¸ºFalseï¼Œå¦‚æœæŸä¸€ä¸ªvariableçš„volatileå±æ€§è¢«è®¾ä¸ºTrueï¼Œé‚£ä¹ˆæ‰€æœ‰ä¾èµ–å®ƒçš„èŠ‚ç‚¹volatileå±æ€§éƒ½ä¸ºTrueã€‚volatileå±æ€§ä¸ºTrueçš„èŠ‚ç‚¹ä¸ä¼šæ±‚å¯¼ï¼Œvolatileçš„ä¼˜å…ˆçº§æ¯”requires_gradé«˜ã€‚
    5.å¤šæ¬¡åå‘ä¼ æ’­æ—¶ï¼Œæ¢¯åº¦æ˜¯ç´¯åŠ çš„ã€‚åå‘ä¼ æ’­çš„ä¸­é—´ç¼“å­˜ä¼šè¢«æ¸…ç©ºï¼Œä¸ºè¿›è¡Œå¤šæ¬¡åå‘ä¼ æ’­éœ€æŒ‡å®šretain_graph=Trueæ¥ä¿å­˜è¿™äº›ç¼“å­˜ã€‚
    6.éå¶å­èŠ‚ç‚¹çš„æ¢¯åº¦è®¡ç®—å®Œä¹‹åå³è¢«æ¸…ç©ºï¼Œå¯ä»¥ä½¿ç”¨autograd.gradæˆ–hookæŠ€æœ¯è·å–éå¶å­èŠ‚ç‚¹çš„å€¼ã€‚
    7.variableçš„gradä¸dataå½¢çŠ¶ä¸€è‡´ï¼Œåº”é¿å…ç›´æ¥ä¿®æ”¹variable.dataï¼Œå› ä¸ºå¯¹dataçš„ç›´æ¥æ“ä½œæ— æ³•åˆ©ç”¨autogradè¿›è¡Œåå‘ä¼ æ’­
    8.åå‘ä¼ æ’­å‡½æ•°backwardçš„å‚æ•°grad_variableså¯ä»¥çœ‹æˆé“¾å¼æ±‚å¯¼çš„ä¸­é—´ç»“æœï¼Œå¦‚æœæ˜¯æ ‡é‡ï¼Œå¯ä»¥çœç•¥ï¼Œé»˜è®¤ä¸º1
    9.PyTorché‡‡ç”¨åŠ¨æ€å›¾è®¾è®¡ï¼Œå¯ä»¥å¾ˆæ–¹ä¾¿åœ°æŸ¥çœ‹ä¸­é—´å±‚çš„è¾“å‡ºï¼ŒåŠ¨æ€çš„è®¾è®¡è®¡ç®—å›¾ç»“æ„ã€‚
"""

""" loss.backward(grad_loss ) çš„æƒ…å†µ """
def f(x):
    '''è®¡ç®—y'''
    y = x**2 * torch.exp(x)
    return y
x = torch.randn(3,4, requires_grad = True)
y = f(x)
print(y)
# ç»“æœï¼š
# tensor([[1.6681e-01, 2.9650e+00, 9.1634e+00, 4.9143e-01],
#         [7.4560e-02, 3.3950e+00, 1.8273e+01, 2.8271e-01],
#         [7.8892e+00, 4.2957e-04, 4.1004e-01, 1.2708e-02]], grad_fn=<MulBackward0>)

# y.backward() å¦‚æœä»¥æ­¤æ‰§è¡Œ backward()ï¼Œæ—  x.grad ç»“æœã€‚ å³ backward() å‡½æ•°ä¸­ï¼Œå‚æ•° grad_variables å‡ºé”™ã€‚
# å‚æ•° grad_variables åº”ä¸º ç»“æœå€¼ å¯¹ å½“å‰å€¼ çš„ å¯¼æ•°ã€‚ ç›¸å½“äº dy/dyã€‚ ç»“æœä¸º 1 ï¼Œä½†å½¢çŠ¶ä¸º y.size()

y.backward(torch.ones(y.size()),retain_graph=True) # gradientå½¢çŠ¶ä¸yä¸€è‡´
# t.ones(y.size())ç›¸å½“äºgrad_variablesï¼šå½¢çŠ¶ä¸variableä¸€è‡´ï¼Œå¯¹äºy.backward()ï¼Œgrad_variablesç›¸å½“äºé“¾å¼æ³•åˆ™ ğ‘‘ğ‘§ğ‘‘ğ‘¥=ğ‘‘ğ‘§ğ‘‘ğ‘¦Ã—ğ‘‘ğ‘¦ğ‘‘ğ‘¥ ä¸­çš„ ğğ³ğğ² ã€‚
# ä¸èƒ½å•ç‹¬è¿è¡Œä¸¤æ¬¡ RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.
print(x.grad)

# å¦‚æœ ï¼š
z = torch.sum(y) # å•ç‹¬çš„ sumå‡½æ•° ä¸è¡Œï¼Œéœ€è¦ ä½¿ç”¨ torch.sum() å› ä¸º è¿˜æœ‰ grad_fn ç­‰åå‘ä¼ æ’­å‡½æ•°ã€‚
z.backward() # æ­¤æ—¶çš„ grad_variables =dz/dz ï¼Œå› ä¸º z æ˜¯æ ‡é‡ï¼Œ dz/dz =1 ,å¯ä»¥çœç•¥ä¸å†™ã€‚
print(x.grad)

""" è®¡ç®—å›¾ çš„ ç ”ç©¶ã€‚"""
import torch as t
x = t.ones(1)
b = t.rand(1, requires_grad = True)
w = t.rand(1, requires_grad = True)
y = (w*3) * x**2 # ç­‰ä»·äºy=w.mul(x)

# y.requires_grad = False
# RuntimeError: you can only change requires_grad flags of leaf variables.

z = y + b # ç­‰ä»·äºz=y.add(b)

y.backward()
# y.grad ä¸­é—´èŠ‚ç‚¹ y æ—  grad
print(w.grad)
print('x.grad',x.grad)
print(b.grad)

print('x.requires_gradï¼š',x.requires_grad)
print('b.requires_gradï¼š',b.requires_grad)
print('w.requires_gradï¼š',w.requires_grad)
print('y.requires_gradï¼š',y.requires_grad) # è™½ç„¶æœªæŒ‡å®š y.requires_gradä¸ºTrueï¼Œä½†å› ä¸º y.grad çš„è®¡ç®— éœ€è¦ w ï¼Œè€Œ w.requires_grad=True

""" next_functionsä¿å­˜grad_fnçš„è¾“å…¥ï¼Œæ˜¯ä¸€ä¸ªtupleï¼Œtupleçš„å…ƒç´ ä¹Ÿæ˜¯Function
    ç¬¬ä¸€ä¸ªæ˜¯yï¼Œå®ƒæ˜¯ä¹˜æ³•(mul)çš„è¾“å‡ºï¼Œæ‰€ä»¥å¯¹åº”çš„åå‘ä¼ æ’­å‡½æ•°y.grad_fnæ˜¯MulBackward
    ç¬¬äºŒä¸ªæ˜¯bï¼Œå®ƒæ˜¯å¶å­èŠ‚ç‚¹ï¼Œç”±ç”¨æˆ·åˆ›å»ºï¼Œgrad_fnä¸ºNoneï¼Œä½†æ˜¯æœ‰
"""
print('z.grad_fn.next_functions:',z.grad_fn.next_functions) # è®¡ç®—å›¾ä¸­ å¯¹åº”çš„ åå‘ä¼ æ’­ æ— ç¯æœ‰å‘å›¾ã€‚

# variableçš„grad_fnå¯¹åº”ç€å’Œå›¾ä¸­çš„functionç›¸å¯¹åº”
print('z.grad_fn.next_functions[0][0] == y.grad_fn :',z.grad_fn.next_functions[0][0] == y.grad_fn)

print('z.grad_fn.next_functions[0][0].next_functions:',z.grad_fn.next_functions[0][0].next_functions)
print('y.grad_fn.next_functions:',y.grad_fn.next_functions)


""" å…³é—­è‡ªåŠ¨æ±‚å¯¼åŠŸèƒ½ã€‚
    æœ‰äº›æ—¶å€™æˆ‘ä»¬å¯èƒ½ä¸å¸Œæœ›autogradå¯¹tensoræ±‚å¯¼ã€‚è®¤ä¸ºæ±‚å¯¼éœ€è¦ç¼“å­˜è®¸å¤šä¸­é—´ç»“æ„ï¼Œå¢åŠ é¢å¤–çš„å†…å­˜/æ˜¾å­˜å¼€é”€ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å…³é—­è‡ªåŠ¨æ±‚å¯¼ã€‚å¯¹äºä¸éœ€è¦åå‘ä¼ æ’­çš„æƒ…æ™¯ï¼ˆå¦‚inferenceï¼Œå³æµ‹è¯•æ¨ç†æ—¶ï¼‰ï¼Œ
    å…³é—­è‡ªåŠ¨æ±‚å¯¼å¯å®ç°ä¸€å®šç¨‹åº¦çš„é€Ÿåº¦æå‡ï¼Œå¹¶èŠ‚çœçº¦ä¸€åŠæ˜¾å­˜ï¼Œå› å…¶ä¸éœ€è¦åˆ†é…ç©ºé—´è®¡ç®—æ¢¯åº¦ã€‚
"""

with t.no_grad():
    x = t.ones(1)
    w = t.rand(1, requires_grad = True)
    y = x * w
# yä¾èµ–äºwå’Œxï¼Œè™½ç„¶w.requires_grad = Trueï¼Œä½†æ˜¯yçš„requires_gradä¾æ—§ä¸ºFalse
print('x.requires_grad:',x.requires_grad)
print('w.requires_grad:',w.requires_grad)
print('y.requires_grad:',y.requires_grad) # å¯ä»¥çœ‹åˆ°  # y.requires_grad: False
# æˆ–è€… é€šè¿‡ t.set_grad_enabled(False) è®¾ç½® ï¼Œå¹¶é€šè¿‡ t.set_grad_enabled(True) æ¢å¤ã€‚

"""åªè¦ä½ å¯¹éœ€è¦æ±‚å¯¼çš„å¶å­å¼ é‡ä½¿ç”¨äº†è¿™äº›æ“ä½œï¼Œé©¬ä¸Šå°±ä¼šæŠ¥é”™"""
"""æ‰€è°“åŠ¨æ€å›¾ï¼Œå°±æ˜¯æ¯æ¬¡å½“æˆ‘ä»¬æ­å»ºå®Œä¸€ä¸ªè®¡ç®—å›¾ï¼Œç„¶ååœ¨åå‘ä¼ æ’­ç»“æŸä¹‹åï¼Œæ•´ä¸ªè®¡ç®—å›¾å°±åœ¨å†…å­˜ä¸­è¢«é‡Šæ”¾äº†ã€‚å¦‚æœæƒ³å†æ¬¡ä½¿ç”¨çš„è¯ï¼Œå¿…é¡»ä»å¤´å†æ­ä¸€éï¼Œ"""