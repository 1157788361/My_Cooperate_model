## my idea
* 1.standard loss 加上 gt_recon_loss √

* 2.在 encoder_decoder.py 中 将 class Dual_Branch_Encoder(439行) 
   添加 self.code_decoupler_multi， 探索解耦器的性能或复杂度对模型的影响 √ 
   
* 3.在 train_adv_supervised_segmentation_triplet.py 中增加 eval_model_iter_N() 函数，
探索 STN性能对模型最后精度的影响。 √ 

* 4.使用类似于 递归网络， 对不同 隐藏层 进行掩码，或加强，查看效果。（主要针对目标中心和目标周围）

* 5.因为原文掩码针对 梯度较大的位置，可视化其在 原图像中的感受野， 
  查看不同隐藏层的掩码主要作用于原图的什么位置。

* 6.实验室 在 label_1 的周围 错误的 包含了厚度为 1 的 label_2,影响实验效果。
   如果人为 将 其他label围绕于目标label，是否会对实验性能有所改善。

* 7.考虑逐层向下的 反向注意力 或 与 逐层隐藏层掩码进行结合，主要体现出 逐层 与 渐进

* 8.快网络 与 慢网络 的迭代步长区分，如 快 2 与 慢 1 

* 9.Grad-CAM 中 对于 最后一层的feature map(A^K)，使用 linear combination ，可以考虑加入注意力机制。
因为针对的 是 多层feature map 的复杂结合。也即 L(c/Grad-CAM) 计算

* 10.双 label ?，原 label 和需要注意的位置的label。（有点类似于 Grad-CAM 中的，反事实解释。）
如果将一个图像的输入对应两个label，一个事实label,一个反事实label。会不会在 区分类与类之间的效果好一点。

* 11.掩码的目标从 单一的梯度大小，到 含有类别权重的 各 feature map 的 梯度。 

* 12.解耦器处，利用 注意力进行。
******************************************************************************
## TODO：
- 1.找出模型预测时，哪些地方预测失误，哪些地方未预测到。 √

- 2.找出网络响应最大的位置。 

  > 考虑：利用交叉熵热图。

- 3.反向注意机制与边界信息。

- 4.掩码不同隐藏层。
********************************************************************************
## experiment 
> ### 数据集 illness
> #### 针对 解耦器 decoupler 的探索
> |1_conv,no_SN|2_conv,no_SN(origin)| 3_conv,no_SN | 4_conv,no_SN|2_conv,SN|None_decoupler
> :---------:|:---------:|:----------:|:----------:|:----------:|:----------:|
> pass|pass|pass|pass|pass|pass





 | aaa | 小黑 | 小白 |
 | :------ | :-------- | :---------- |
 |拉拉 |算法 |双方都|
 